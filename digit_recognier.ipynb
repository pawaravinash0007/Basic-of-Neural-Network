{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40dT2rG7Adpf"
      },
      "source": [
        "Unzip file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "https://www.kaggle.com/competitions/digit-recognizer/data"
      ],
      "metadata": {
        "id": "5ij4P4sX8C_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Se_ChDQ8w1j",
        "outputId": "53fb334d-b46a-4c0d-9d51-61e40433c611"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Basic of Neural Network/digit-recognizer.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxhkVk_17F53",
        "outputId": "e4b405c3-c341-4aef-c421-58a819d8e86d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Basic of Neural Network/digit-recognizer.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9xT90PvPAnNX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBIxXTRjBvtn"
      },
      "source": [
        "aapproch\n",
        "1. load the libraries and data\n",
        "2. visualize the data\n",
        "3. divide the data intoX(Images) and Y(Labels)\n",
        "4. Divide the train data into train and test\n",
        "5. Apply DT and Random statement\n",
        "6. Evaluate the Dtree and RF model\n",
        "7. Apply single layer perception\n",
        "8. Evaluate the SLP\n",
        "9. Compare the results between ML (Dtree and RF)\n",
        "10. Draw conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqFygv8CrBw"
      },
      "source": [
        "1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MlU0Sd_RCZWv"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test=pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i7rMXJq3C4xt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ed3e4252-8a26-4b49-cdf0-c62abd9a3008"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
              "0      1       0       0       0       0       0       0       0       0   \n",
              "1      0       0       0       0       0       0       0       0       0   \n",
              "2      1       0       0       0       0       0       0       0       0   \n",
              "3      4       0       0       0       0       0       0       0       0   \n",
              "4      0       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
              "0       0  ...         0         0         0         0         0         0   \n",
              "1       0  ...         0         0         0         0         0         0   \n",
              "2       0  ...         0         0         0         0         0         0   \n",
              "3       0  ...         0         0         0         0         0         0   \n",
              "4       0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel780  pixel781  pixel782  pixel783  \n",
              "0         0         0         0         0  \n",
              "1         0         0         0         0  \n",
              "2         0         0         0         0  \n",
              "3         0         0         0         0  \n",
              "4         0         0         0         0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23cf0f98-c994-41b0-84d9-658ab56ff5de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23cf0f98-c994-41b0-84d9-658ab56ff5de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23cf0f98-c994-41b0-84d9-658ab56ff5de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23cf0f98-c994-41b0-84d9-658ab56ff5de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-50b46ae8-3da1-42a1-95db-ae07a8936bc2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50b46ae8-3da1-42a1-95db-ae07a8936bc2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-50b46ae8-3da1-42a1-95db-ae07a8936bc2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1qaGjFDZC64m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3af59ad-0efd-4e7e-c766-5a19030b9cec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'parse matrix\\nsparsity is big issue'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"parse matrix\n",
        "sparsity is big issue\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BukbKXaxDk24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "415851e2-5592-414a-8547-e1e2c09f1f46"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Extract pixel values for the first image\n",
        "first_image_pixels = train.iloc[0, 1:].values  # Exclude the label column (first column)\n",
        "\n",
        "# Reshape the pixel values into a 28x28 image\n",
        "image = first_image_pixels.reshape(28, 28)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title('First Image from Train Data')\n",
        "plt.show()\n",
        "\n",
        "# You can also print the pixel values if needed:\n",
        "# print(first_image_pixels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s2hxxfHKFPne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb78151-ac32-4f60-8e18-cc630668b052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ar2rfX6gF1Di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe018e8-5dbe-4acd-8c8a-63e3a42c5d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000,)\n"
          ]
        }
      ],
      "source": [
        "y=train[\"label\"]\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VVN2rV8HF1AK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc15e039-64c5-4194-ba8f-563bc1affaf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 784)\n"
          ]
        }
      ],
      "source": [
        "x=train.drop(\"label\",axis=1)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BnGtht87F09N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "73c76278-a52a-4467-b1f8-5f5ae2e0ab65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pixel0        0.000000\n",
              "pixel1        0.000000\n",
              "pixel2        0.000000\n",
              "pixel3        0.000000\n",
              "pixel4        0.000000\n",
              "               ...    \n",
              "pixel779    145.149671\n",
              "pixel780      0.000000\n",
              "pixel781      0.000000\n",
              "pixel782      0.000000\n",
              "pixel783      0.000000\n",
              "Length: 784, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pixel0</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel1</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel2</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel3</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel4</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel779</th>\n",
              "      <td>145.149671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel780</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel781</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel782</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pixel783</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>784 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x.skew()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh6eFJsHHPhb"
      },
      "source": [
        "Divide data in to train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vzJu0GVIF06q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafb9b25-7dc6-4bc4-c4c7-3bb41555a0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x train (33600, 784)\n",
            "x test (8400, 784)\n",
            "y train (33600,)\n",
            "y train (8400,)\n"
          ]
        }
      ],
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=23)\n",
        "print(f\"x train {x_train.shape}\")\n",
        "print(f\"x test {x_test.shape}\")\n",
        "print(f\"y train {y_train.shape}\")\n",
        "print(f\"y train {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq0W3XQQJAVm"
      },
      "source": [
        "decision tree\n",
        "random forest\n",
        "grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA2q1KlsgVrn"
      },
      "source": [
        "1. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gk2mFFD_F038"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dQnebuodhOtL"
      },
      "outputs": [],
      "source": [
        "acc=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BEImmJPlF01N"
      },
      "outputs": [],
      "source": [
        "logistic_model=LogisticRegression()\n",
        "logistic_model.fit(x_train,y_train)\n",
        "y_log_pr=logistic_model.predict(x_test)\n",
        "acc.append(accuracy_score(y_test,y_log_pr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7B1KXmglF0yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8f6f38-aaec-47d0-b6e9-9336fca85f93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9142857142857143]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGcTW1aDho_G"
      },
      "source": [
        "2. Decision Tree Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wiaZCbjfF0vw"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KjPTW-swF0tB"
      },
      "outputs": [],
      "source": [
        "dtc_model=DecisionTreeClassifier()\n",
        "dtc_model.fit(x_train,y_train)\n",
        "y_dtc_pr=dtc_model.predict(x_test)\n",
        "acc.append(accuracy_score(y_test,y_dtc_pr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-ZKgvhZYh5SU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077057a8-d49d-4ce2-d0f7-31f467f671d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9142857142857143, 0.8596428571428572]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-1LfQc-iU40"
      },
      "source": [
        "3. Random Forest Regressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4ukyMs0niQEC"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "66AJEuBtif8L"
      },
      "outputs": [],
      "source": [
        "rf_model=RandomForestClassifier()\n",
        "rf_model.fit(x_train,y_train)\n",
        "y_rf_pr=rf_model.predict(x_test)\n",
        "acc.append(accuracy_score(y_test,y_rf_pr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tZFc5WubiwY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6062619b-6613-4a2c-b81f-7bc42589b90e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9142857142857143, 0.8596428571428572, 0.9639285714285715]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iiepO4G6i-5b"
      },
      "outputs": [],
      "source": [
        "# param_grid={\n",
        "#    \"n_estimators\":[100,200,300,400,500],\n",
        "#    \"max_depth\":[10,20,30,40.50],\n",
        "#    \"min_samples_split\":[20,30,40,50,60],\n",
        "#    \"min_samples_leaf\":[2,4,6,8,10,12]\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HaRHqwg_jhYx"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# grid_search=GridSearchCV(estimator=rf_model,param_grid=param_grid,n_jobs=-1)\n",
        "# grid_search.fit(x_train,y_train)\n",
        "# best_param=grid_search.best_params_\n",
        "# best_model=grid_search.best_estimator_\n",
        "# print(best_param)\n",
        "# print(best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p60yUVlvkA1n"
      },
      "source": [
        "Navies Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K5lxUUlwjlue"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "navies_model= GaussianNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "thwxgY_bjlrH"
      },
      "outputs": [],
      "source": [
        "navies_model.fit(x_train,y_train)\n",
        "y_pred_nb=navies_model.predict(x_test)\n",
        "acc.append(accuracy_score(y_test,y_pred_nb))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj0zkopG3ySv",
        "outputId": "cbc230e3-f6c8-43e1-bd7a-89da60b5c82b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9142857142857143,\n",
              " 0.8596428571428572,\n",
              " 0.9639285714285715,\n",
              " 0.5497619047619048]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XQELLmAZjlk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7194cb24-6ee5-4341-9897-c94ed2d6ff2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      3279\n",
            "           1       1.00      1.00      1.00      3760\n",
            "           2       1.00      1.00      1.00      3320\n",
            "           3       1.00      1.00      1.00      3472\n",
            "           4       1.00      1.00      1.00      3262\n",
            "           5       1.00      1.00      1.00      3051\n",
            "           6       1.00      1.00      1.00      3300\n",
            "           7       1.00      1.00      1.00      3543\n",
            "           8       1.00      1.00      1.00      3272\n",
            "           9       1.00      1.00      1.00      3341\n",
            "\n",
            "    accuracy                           1.00     33600\n",
            "   macro avg       1.00      1.00      1.00     33600\n",
            "weighted avg       1.00      1.00      1.00     33600\n",
            "\n",
            "[[3279    0    0    0    0    0    0    0    0    0]\n",
            " [   0 3760    0    0    0    0    0    0    0    0]\n",
            " [   0    0 3320    0    0    0    0    0    0    0]\n",
            " [   0    0    0 3472    0    0    0    0    0    0]\n",
            " [   0    0    0    0 3262    0    0    0    0    0]\n",
            " [   0    0    0    0    0 3051    0    0    0    0]\n",
            " [   0    0    0    0    0    0 3300    0    0    0]\n",
            " [   0    0    0    0    0    0    0 3543    0    0]\n",
            " [   0    0    0    0    0    0    0    0 3272    0]\n",
            " [   0    0    0    0    0    0    0    0    0 3341]]\n",
            "Accuracy Score: 1.0\n",
            "\n",
            "Testing Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       853\n",
            "           1       0.98      0.98      0.98       924\n",
            "           2       0.96      0.96      0.96       857\n",
            "           3       0.95      0.95      0.95       879\n",
            "           4       0.96      0.96      0.96       810\n",
            "           5       0.97      0.97      0.97       744\n",
            "           6       0.97      0.98      0.98       837\n",
            "           7       0.96      0.96      0.96       858\n",
            "           8       0.96      0.94      0.95       791\n",
            "           9       0.94      0.95      0.94       847\n",
            "\n",
            "    accuracy                           0.96      8400\n",
            "   macro avg       0.96      0.96      0.96      8400\n",
            "weighted avg       0.96      0.96      0.96      8400\n",
            "\n",
            "[[841   0   0   1   1   1   7   0   2   0]\n",
            " [  0 909   4   4   1   1   2   1   2   0]\n",
            " [  4   1 826   5   2   0   1  10   6   2]\n",
            " [  2   3  11 836   1   4   2   9   7   4]\n",
            " [  1   1   1   0 776   1   4   1   4  21]\n",
            " [  2   1   1   8   2 720   4   1   2   3]\n",
            " [  6   0   0   0   1   7 821   0   2   0]\n",
            " [  0   5  10   2   2   0   0 822   4  13]\n",
            " [  2   5   4  12   4   6   4   1 745   8]\n",
            " [  5   0   1  12  15   2   0   8   3 801]]\n",
            "Accuracy Score: 0.9639285714285715\n"
          ]
        }
      ],
      "source": [
        "def model_evaluation(model, x_train, y_train, x_test, y_test):\n",
        "    y_hat_train = model.predict(x_train)\n",
        "    y_hat_test = model.predict(x_test)\n",
        "\n",
        "    print(\"Training Data:\")\n",
        "    print(classification_report(y_train, y_hat_train))\n",
        "    print(confusion_matrix(y_train, y_hat_train))\n",
        "    print(f\"Accuracy Score: {accuracy_score(y_train, y_hat_train)}\")\n",
        "\n",
        "    print(\"\\nTesting Data:\")\n",
        "    print(classification_report(y_test, y_hat_test))\n",
        "    print(confusion_matrix(y_test, y_hat_test))\n",
        "    print(f\"Accuracy Score: {accuracy_score(y_test, y_hat_test)}\")\n",
        "\n",
        "# Example usage (replace with your actual model and data)\n",
        "model_evaluation(rf_model, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Layer Perceptron (Single Neuron)"
      ],
      "metadata": {
        "id": "cExFLjaO98P3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kWqzihSLzxEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8d9a47-3d26-4d59-c7fc-d6976cd8f850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 34637.94, NNZs: 607, Bias: -90.000000, T: 33600, Avg. loss: 48395.747768\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41825.28, NNZs: 618, Bias: -151.000000, T: 67200, Avg. loss: 40090.612708\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48393.07, NNZs: 621, Bias: -212.000000, T: 100800, Avg. loss: 39643.426429\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 54024.41, NNZs: 625, Bias: -275.000000, T: 134400, Avg. loss: 38581.834911\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56698.46, NNZs: 626, Bias: -323.000000, T: 168000, Avg. loss: 36739.054762\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60821.16, NNZs: 626, Bias: -376.000000, T: 201600, Avg. loss: 35937.408810\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 64127.84, NNZs: 629, Bias: -434.000000, T: 235200, Avg. loss: 36095.758750\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 68767.20, NNZs: 629, Bias: -493.000000, T: 268800, Avg. loss: 36200.006458\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 72245.29, NNZs: 631, Bias: -537.000000, T: 302400, Avg. loss: 34592.925565\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 75230.93, NNZs: 631, Bias: -588.000000, T: 336000, Avg. loss: 36088.619702\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 78287.54, NNZs: 632, Bias: -642.000000, T: 369600, Avg. loss: 34866.103214\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 81027.99, NNZs: 634, Bias: -696.000000, T: 403200, Avg. loss: 35915.722024\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 83395.41, NNZs: 634, Bias: -743.000000, T: 436800, Avg. loss: 32406.707321\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 85224.51, NNZs: 634, Bias: -785.000000, T: 470400, Avg. loss: 33046.579821\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 87016.87, NNZs: 635, Bias: -832.000000, T: 504000, Avg. loss: 34713.784643\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 89030.44, NNZs: 635, Bias: -882.000000, T: 537600, Avg. loss: 34403.438244\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 90691.66, NNZs: 635, Bias: -926.000000, T: 571200, Avg. loss: 32657.604315\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 92622.15, NNZs: 635, Bias: -968.000000, T: 604800, Avg. loss: 35311.924673\n",
            "Total training time: 1.27 seconds.\n",
            "Convergence after 18 epochs took 1.27 seconds\n",
            "-- Epoch 1\n",
            "Norm: 25650.38, NNZs: 559, Bias: -28.000000, T: 33600, Avg. loss: 25129.263304\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 31234.42, NNZs: 578, Bias: -50.000000, T: 67200, Avg. loss: 22016.920446\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 35019.17, NNZs: 588, Bias: -70.000000, T: 100800, Avg. loss: 22492.050149\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 38484.84, NNZs: 593, Bias: -84.000000, T: 134400, Avg. loss: 21289.514613\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 41890.35, NNZs: 594, Bias: -102.000000, T: 168000, Avg. loss: 20734.373333\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44860.21, NNZs: 597, Bias: -110.000000, T: 201600, Avg. loss: 20528.080268\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 47616.68, NNZs: 598, Bias: -117.000000, T: 235200, Avg. loss: 20218.247857\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 50258.89, NNZs: 601, Bias: -128.000000, T: 268800, Avg. loss: 19146.019048\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 52333.20, NNZs: 604, Bias: -140.000000, T: 302400, Avg. loss: 19408.616012\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 55203.76, NNZs: 608, Bias: -150.000000, T: 336000, Avg. loss: 18927.184405\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 57336.01, NNZs: 609, Bias: -158.000000, T: 369600, Avg. loss: 18466.342292\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 58889.94, NNZs: 609, Bias: -168.000000, T: 403200, Avg. loss: 18839.183690\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60960.06, NNZs: 610, Bias: -176.000000, T: 436800, Avg. loss: 18543.922560\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 62735.78, NNZs: 610, Bias: -187.000000, T: 470400, Avg. loss: 18905.126607\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 64811.08, NNZs: 610, Bias: -196.000000, T: 504000, Avg. loss: 18216.614018\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 66786.01, NNZs: 610, Bias: -204.000000, T: 537600, Avg. loss: 17563.855119\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 68191.28, NNZs: 610, Bias: -212.000000, T: 571200, Avg. loss: 16844.261012\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 69963.76, NNZs: 608, Bias: -222.000000, T: 604800, Avg. loss: 17673.967381\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 71770.50, NNZs: 610, Bias: -228.000000, T: 638400, Avg. loss: 16955.705238\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 72907.37, NNZs: 616, Bias: -240.000000, T: 672000, Avg. loss: 17756.255000\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 74965.18, NNZs: 616, Bias: -248.000000, T: 705600, Avg. loss: 17488.330060\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 76696.60, NNZs: 616, Bias: -261.000000, T: 739200, Avg. loss: 17064.395863\n",
            "Total training time: 1.46 seconds.\n",
            "Convergence after 22 epochs took 1.46 seconds\n",
            "-- Epoch 1\n",
            "Norm: 36129.66, NNZs: 617, Bias: -103.000000, T: 33600, Avg. loss: 96350.902232\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42715.88, NNZs: 624, Bias: -179.000000, T: 67200, Avg. loss: 91240.941875\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48550.16, NNZs: 632, Bias: -236.000000, T: 100800, Avg. loss: 90385.698185\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52912.87, NNZs: 632, Bias: -296.000000, T: 134400, Avg. loss: 90175.116280\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56250.99, NNZs: 640, Bias: -358.000000, T: 168000, Avg. loss: 88223.357321\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59463.51, NNZs: 643, Bias: -410.000000, T: 201600, Avg. loss: 91690.213839\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 64016.07, NNZs: 643, Bias: -474.000000, T: 235200, Avg. loss: 87409.815000\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65573.38, NNZs: 644, Bias: -529.000000, T: 268800, Avg. loss: 89273.644732\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 68666.62, NNZs: 645, Bias: -590.000000, T: 302400, Avg. loss: 86024.941488\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 70648.29, NNZs: 647, Bias: -646.000000, T: 336000, Avg. loss: 90362.726935\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 72021.87, NNZs: 647, Bias: -708.000000, T: 369600, Avg. loss: 89111.502738\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 75028.70, NNZs: 649, Bias: -762.000000, T: 403200, Avg. loss: 87183.324315\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 76914.09, NNZs: 653, Bias: -831.000000, T: 436800, Avg. loss: 89518.425952\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 78414.02, NNZs: 656, Bias: -881.000000, T: 470400, Avg. loss: 86255.998839\n",
            "Total training time: 0.77 seconds.\n",
            "Convergence after 14 epochs took 0.77 seconds\n",
            "-- Epoch 1\n",
            "Norm: 36167.67, NNZs: 586, Bias: -172.000000, T: 33600, Avg. loss: 129957.056280\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42308.50, NNZs: 599, Bias: -326.000000, T: 67200, Avg. loss: 127865.645000\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 46712.67, NNZs: 605, Bias: -463.000000, T: 100800, Avg. loss: 124314.379256\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52143.34, NNZs: 606, Bias: -607.000000, T: 134400, Avg. loss: 121874.329226\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 55190.08, NNZs: 611, Bias: -752.000000, T: 168000, Avg. loss: 125234.753363\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 58881.60, NNZs: 613, Bias: -888.000000, T: 201600, Avg. loss: 118852.436429\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 61179.37, NNZs: 618, Bias: -1014.000000, T: 235200, Avg. loss: 122520.111220\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 62253.89, NNZs: 618, Bias: -1137.000000, T: 268800, Avg. loss: 123065.118452\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 65469.68, NNZs: 618, Bias: -1265.000000, T: 302400, Avg. loss: 123353.712560\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 68270.22, NNZs: 620, Bias: -1388.000000, T: 336000, Avg. loss: 123120.910833\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 70579.87, NNZs: 623, Bias: -1526.000000, T: 369600, Avg. loss: 122285.982768\n",
            "Total training time: 0.60 seconds.\n",
            "Convergence after 11 epochs took 0.60 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37707.08, NNZs: 631, Bias: -55.000000, T: 33600, Avg. loss: 71917.918065\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46212.83, NNZs: 642, Bias: -103.000000, T: 67200, Avg. loss: 65673.791190\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 51388.15, NNZs: 656, Bias: -144.000000, T: 100800, Avg. loss: 63859.985298\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56598.67, NNZs: 655, Bias: -173.000000, T: 134400, Avg. loss: 60630.895982\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 61030.98, NNZs: 656, Bias: -206.000000, T: 168000, Avg. loss: 61245.047798\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 64984.89, NNZs: 656, Bias: -236.000000, T: 201600, Avg. loss: 58485.050774\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 68799.49, NNZs: 659, Bias: -265.000000, T: 235200, Avg. loss: 57871.996161\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 72424.54, NNZs: 659, Bias: -292.000000, T: 268800, Avg. loss: 57622.399435\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 74231.05, NNZs: 661, Bias: -319.000000, T: 302400, Avg. loss: 60507.003869\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 77259.89, NNZs: 660, Bias: -356.000000, T: 336000, Avg. loss: 56145.182589\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 79893.39, NNZs: 661, Bias: -383.000000, T: 369600, Avg. loss: 57602.108155\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82835.69, NNZs: 663, Bias: -410.000000, T: 403200, Avg. loss: 56910.736458\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 84369.10, NNZs: 667, Bias: -449.000000, T: 436800, Avg. loss: 57689.269524\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 87171.30, NNZs: 667, Bias: -475.000000, T: 470400, Avg. loss: 58125.314196\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 89470.61, NNZs: 667, Bias: -510.000000, T: 504000, Avg. loss: 58649.906607\n",
            "Total training time: 0.81 seconds.\n",
            "Convergence after 15 epochs took 0.81 seconds\n",
            "-- Epoch 1\n",
            "Norm: 41986.43, NNZs: 619, Bias: 35.000000, T: 33600, Avg. loss: 131973.133006\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50469.83, NNZs: 628, Bias: 61.000000, T: 67200, Avg. loss: 120805.386845\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 56017.40, NNZs: 635, Bias: 93.000000, T: 100800, Avg. loss: 117305.975506\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 61562.51, NNZs: 637, Bias: 132.000000, T: 134400, Avg. loss: 119466.001518\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 64822.93, NNZs: 637, Bias: 160.000000, T: 168000, Avg. loss: 116879.955744\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 68301.29, NNZs: 637, Bias: 184.000000, T: 201600, Avg. loss: 114814.878542\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 71836.39, NNZs: 638, Bias: 213.000000, T: 235200, Avg. loss: 116422.594315\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 74172.29, NNZs: 638, Bias: 253.000000, T: 268800, Avg. loss: 111183.589583\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 76197.26, NNZs: 638, Bias: 291.000000, T: 302400, Avg. loss: 113220.335119\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 80315.54, NNZs: 639, Bias: 323.000000, T: 336000, Avg. loss: 116969.911696\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 80969.71, NNZs: 639, Bias: 361.000000, T: 369600, Avg. loss: 115677.498185\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82953.56, NNZs: 639, Bias: 391.000000, T: 403200, Avg. loss: 114914.649226\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 85237.52, NNZs: 639, Bias: 443.000000, T: 436800, Avg. loss: 112837.464345\n",
            "Total training time: 0.72 seconds.\n",
            "Convergence after 13 epochs took 0.72 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33773.12, NNZs: 594, Bias: -88.000000, T: 33600, Avg. loss: 63692.618065\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41119.95, NNZs: 599, Bias: -161.000000, T: 67200, Avg. loss: 58411.548750\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 44973.93, NNZs: 602, Bias: -240.000000, T: 100800, Avg. loss: 57237.226845\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48937.94, NNZs: 605, Bias: -317.000000, T: 134400, Avg. loss: 53824.293899\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 52063.92, NNZs: 605, Bias: -377.000000, T: 168000, Avg. loss: 53988.610476\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 55412.86, NNZs: 604, Bias: -433.000000, T: 201600, Avg. loss: 52224.443780\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 58017.58, NNZs: 607, Bias: -499.000000, T: 235200, Avg. loss: 54134.439940\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 60653.65, NNZs: 611, Bias: -566.000000, T: 268800, Avg. loss: 53652.120149\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 63279.67, NNZs: 611, Bias: -635.000000, T: 302400, Avg. loss: 51363.572649\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 65615.13, NNZs: 611, Bias: -698.000000, T: 336000, Avg. loss: 51481.908750\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 68911.46, NNZs: 611, Bias: -768.000000, T: 369600, Avg. loss: 51089.143125\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 71107.01, NNZs: 611, Bias: -836.000000, T: 403200, Avg. loss: 49365.897708\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 72981.11, NNZs: 613, Bias: -892.000000, T: 436800, Avg. loss: 51538.439167\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 73961.41, NNZs: 613, Bias: -956.000000, T: 470400, Avg. loss: 52024.566726\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 75584.47, NNZs: 613, Bias: -1021.000000, T: 504000, Avg. loss: 51584.666042\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 76825.64, NNZs: 613, Bias: -1088.000000, T: 537600, Avg. loss: 51394.788869\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 78657.04, NNZs: 614, Bias: -1156.000000, T: 571200, Avg. loss: 51719.390655\n",
            "Total training time: 0.91 seconds.\n",
            "Convergence after 17 epochs took 0.91 seconds\n",
            "-- Epoch 1\n",
            "Norm: 30853.65, NNZs: 589, Bias: -22.000000, T: 33600, Avg. loss: 62536.357292\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 38037.84, NNZs: 603, Bias: -54.000000, T: 67200, Avg. loss: 57879.632351\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 42282.94, NNZs: 607, Bias: -81.000000, T: 100800, Avg. loss: 56604.923423\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 45944.35, NNZs: 617, Bias: -91.000000, T: 134400, Avg. loss: 56097.342827\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 49672.84, NNZs: 624, Bias: -103.000000, T: 168000, Avg. loss: 55103.730833\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 53152.97, NNZs: 625, Bias: -116.000000, T: 201600, Avg. loss: 55047.447262\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 57234.44, NNZs: 633, Bias: -127.000000, T: 235200, Avg. loss: 52002.182976\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 59074.77, NNZs: 635, Bias: -147.000000, T: 268800, Avg. loss: 54481.130893\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 61841.48, NNZs: 635, Bias: -165.000000, T: 302400, Avg. loss: 52896.081042\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 64144.10, NNZs: 635, Bias: -172.000000, T: 336000, Avg. loss: 52292.392470\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 65361.79, NNZs: 635, Bias: -188.000000, T: 369600, Avg. loss: 52271.613482\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 67914.10, NNZs: 637, Bias: -211.000000, T: 403200, Avg. loss: 52562.103155\n",
            "Total training time: 0.65 seconds.\n",
            "Convergence after 12 epochs took 0.65 seconds\n",
            "-- Epoch 1\n",
            "Norm: 40976.91, NNZs: 613, Bias: -472.000000, T: 33600, Avg. loss: 233552.503065\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 49322.91, NNZs: 617, Bias: -883.000000, T: 67200, Avg. loss: 226879.877887\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54360.38, NNZs: 626, Bias: -1300.000000, T: 100800, Avg. loss: 232040.739643\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 60306.88, NNZs: 628, Bias: -1706.000000, T: 134400, Avg. loss: 220801.912024\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 64677.08, NNZs: 628, Bias: -2109.000000, T: 168000, Avg. loss: 227304.066131\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 67829.09, NNZs: 633, Bias: -2524.000000, T: 201600, Avg. loss: 224740.875565\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 70328.44, NNZs: 635, Bias: -2916.000000, T: 235200, Avg. loss: 231188.102560\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 72321.01, NNZs: 638, Bias: -3306.000000, T: 268800, Avg. loss: 224058.584018\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 74181.43, NNZs: 640, Bias: -3708.000000, T: 302400, Avg. loss: 232193.722440\n",
            "Total training time: 0.49 seconds.\n",
            "Convergence after 9 epochs took 0.49 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37703.84, NNZs: 631, Bias: -217.000000, T: 33600, Avg. loss: 156827.149226\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 45105.65, NNZs: 639, Bias: -396.000000, T: 67200, Avg. loss: 150969.112024\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 51826.66, NNZs: 643, Bias: -565.000000, T: 100800, Avg. loss: 150816.495982\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55393.19, NNZs: 644, Bias: -741.000000, T: 134400, Avg. loss: 148840.409673\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 60219.44, NNZs: 647, Bias: -912.000000, T: 168000, Avg. loss: 149075.286726\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 62730.06, NNZs: 649, Bias: -1073.000000, T: 201600, Avg. loss: 149786.743185\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 65924.15, NNZs: 649, Bias: -1248.000000, T: 235200, Avg. loss: 144964.454196\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 67108.15, NNZs: 649, Bias: -1424.000000, T: 268800, Avg. loss: 150831.783244\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 69168.30, NNZs: 650, Bias: -1589.000000, T: 302400, Avg. loss: 147723.554911\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 72460.87, NNZs: 650, Bias: -1765.000000, T: 336000, Avg. loss: 149167.809643\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 73979.97, NNZs: 650, Bias: -1937.000000, T: 369600, Avg. loss: 150398.579732\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 75732.40, NNZs: 653, Bias: -2094.000000, T: 403200, Avg. loss: 145162.843780\n",
            "Total training time: 0.66 seconds.\n",
            "Convergence after 12 epochs took 0.66 seconds\n"
          ]
        }
      ],
      "source": [
        "per = Perceptron(verbose = 2,random_state=123)\n",
        "per.fit(x_train, y_train)\n",
        "y_hat_train_per = per.predict(x_train)\n",
        "y_hat_test_per = per.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MO5F57NGz0uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744d689a-c23c-4b49-a216-7b53e19279cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "print(per.n_iter_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dob3bMyNz3MX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa069edf-3afc-41ad-faca-4c8f3ea6cda7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "per.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ut4_Gzjn0er5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8d1970-530b-40d6-fbc0-e45712e5b694"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00, -4.0000e+01, -3.8000e+02,\n",
              "       -2.1200e+02,  0.0000e+00, -5.5000e+01,  8.7500e+02,  3.1630e+03,\n",
              "        1.7610e+03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9000e+01,\n",
              "        1.0000e+01,  0.0000e+00, -7.3000e+01, -5.7800e+02, -1.9330e+03,\n",
              "       -3.4730e+03, -2.6090e+03, -5.6540e+03, -5.6230e+03, -3.9910e+03,\n",
              "       -6.7010e+03, -8.2520e+03, -9.1030e+03, -9.0940e+03, -5.8130e+03,\n",
              "       -3.3500e+03, -3.7520e+03, -2.2510e+03, -1.7520e+03, -1.2310e+03,\n",
              "       -5.1200e+02, -5.5000e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  6.8000e+01, -2.0400e+02, -7.6500e+02,\n",
              "       -1.1870e+03, -2.7570e+03, -4.9970e+03, -4.2610e+03, -5.9970e+03,\n",
              "       -4.6610e+03, -6.8690e+03, -1.5830e+03, -3.3330e+03, -3.4410e+03,\n",
              "       -1.4780e+03, -5.0410e+03, -5.3960e+03, -4.8660e+03, -5.1250e+03,\n",
              "       -3.0310e+03, -3.2570e+03, -1.7380e+03, -3.3000e+02, -7.1000e+01,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "       -2.9000e+01, -9.5000e+01, -5.7100e+02, -1.2700e+03, -6.3940e+03,\n",
              "       -6.1490e+03,  2.2390e+03, -7.6880e+03, -1.4500e+02, -4.4000e+01,\n",
              "        1.9950e+03, -2.4040e+03, -6.7500e+02, -8.2920e+03,  2.9870e+03,\n",
              "       -3.6100e+02,  9.6400e+02,  1.2950e+03, -2.2610e+03, -2.4130e+03,\n",
              "       -1.8600e+03, -1.2270e+03, -1.5500e+03, -1.9500e+02,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00, -9.0000e+01, -2.1600e+02, -2.0200e+03,\n",
              "       -2.6130e+03, -7.9000e+01, -4.4600e+03,  1.3610e+03,  3.6630e+03,\n",
              "        3.0560e+03, -2.1130e+03, -1.7700e+03,  2.0890e+03, -3.5200e+02,\n",
              "        2.5960e+03,  1.5420e+03,  1.8980e+03, -3.0430e+03, -2.6890e+03,\n",
              "       -1.0150e+03, -1.6050e+03,  2.3140e+03, -4.0350e+03, -4.7690e+03,\n",
              "       -1.0480e+03, -2.6700e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "       -8.6000e+01, -1.4000e+01, -1.6650e+03,  3.1410e+03,  1.0810e+03,\n",
              "       -3.0170e+03, -1.3530e+03, -4.9550e+03,  2.1930e+03, -2.4180e+03,\n",
              "       -5.3760e+03,  4.4540e+03,  1.3800e+03, -1.2640e+03, -6.2600e+02,\n",
              "       -4.1370e+03,  3.5900e+02,  1.1470e+03, -3.6700e+02,  1.9710e+03,\n",
              "        2.3980e+03,  8.0900e+02, -1.1830e+04, -4.9350e+03, -2.3230e+03,\n",
              "        0.0000e+00,  0.0000e+00, -1.7200e+02,  0.0000e+00, -1.3200e+02,\n",
              "       -1.5640e+03,  3.5960e+03, -6.0900e+02,  3.7660e+03, -3.5890e+03,\n",
              "       -1.4970e+03,  2.7200e+03,  3.2650e+03, -1.1300e+03,  2.9700e+02,\n",
              "       -6.2100e+02,  1.1750e+03, -2.6830e+03,  5.4080e+03,  3.5310e+03,\n",
              "       -4.2400e+02,  3.3180e+03, -4.0210e+03, -1.1240e+03,  2.7340e+03,\n",
              "       -7.0590e+03, -6.5850e+03, -4.8700e+03, -7.3000e+01,  6.8900e+02,\n",
              "        0.0000e+00, -3.3800e+02, -8.1900e+02, -4.2980e+03, -4.4650e+03,\n",
              "       -4.8400e+02, -2.1570e+03, -9.2500e+02,  1.6200e+02, -2.1540e+03,\n",
              "       -4.1900e+02,  3.6360e+03,  1.6200e+02,  2.2570e+03,  4.5510e+03,\n",
              "        5.9990e+03, -5.6700e+02, -2.8300e+02, -2.2090e+03,  1.6680e+03,\n",
              "        6.0200e+02,  3.9820e+03, -1.0820e+03, -8.0550e+03, -7.7950e+03,\n",
              "       -4.7570e+03, -1.2380e+03,  0.0000e+00,  0.0000e+00,  1.2800e+02,\n",
              "       -1.5520e+03, -4.9710e+03, -2.0710e+03, -1.4610e+03, -5.0000e+02,\n",
              "       -1.6490e+03, -1.5300e+03,  6.3450e+03, -2.4570e+03, -4.5590e+03,\n",
              "        3.4950e+03, -1.5250e+03, -3.6050e+03,  1.6520e+03,  1.3830e+03,\n",
              "       -2.1700e+02,  5.4150e+03,  3.5300e+03, -3.2300e+02, -5.7770e+03,\n",
              "        7.6400e+02, -6.0440e+03, -1.0752e+04, -3.8580e+03, -1.0800e+02,\n",
              "        0.0000e+00,  0.0000e+00,  1.9800e+03, -2.3600e+02, -4.1740e+03,\n",
              "        3.5610e+03, -2.9100e+02,  7.2400e+02,  5.3790e+03, -6.6770e+03,\n",
              "        1.1900e+03,  5.1300e+02,  3.3460e+03, -1.8230e+03,  2.6430e+03,\n",
              "        1.1700e+03, -1.6090e+03,  2.8560e+03,  3.5800e+03, -4.1380e+03,\n",
              "        3.1450e+03,  8.8500e+02,  1.0227e+04,  5.5280e+03,  9.4000e+02,\n",
              "       -1.1268e+04, -4.6260e+03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "       -5.1000e+02,  1.7060e+03, -6.1120e+03, -1.1690e+03, -4.7430e+03,\n",
              "        2.4620e+03,  1.1530e+03, -4.8700e+02,  1.2980e+03,  2.6970e+03,\n",
              "       -2.4970e+03, -1.1820e+03, -3.8490e+03, -3.2430e+03, -1.2590e+03,\n",
              "        1.9000e+01, -2.3550e+03,  2.8370e+03,  5.3200e+02,  3.1100e+03,\n",
              "       -2.6180e+03, -3.3000e+02, -3.8260e+03, -1.0324e+04, -5.1310e+03,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2800e+02,  1.1020e+03,\n",
              "       -4.1900e+03,  2.9020e+03, -1.3760e+03, -4.0540e+03, -1.3250e+03,\n",
              "       -2.0600e+03, -3.5400e+03,  3.0030e+03,  8.8800e+02,  1.0900e+02,\n",
              "       -3.0290e+03, -5.3770e+03, -3.5240e+03, -7.4800e+02,  6.9700e+02,\n",
              "        3.9300e+03,  1.0530e+03,  1.6850e+03, -2.7430e+03, -8.1900e+02,\n",
              "        6.6290e+03, -3.6740e+03, -2.9370e+03,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  4.9000e+01, -6.3000e+02, -2.5660e+03, -6.5900e+02,\n",
              "        4.7940e+03, -1.1200e+02,  2.2580e+03,  1.3330e+03,  4.3540e+03,\n",
              "       -4.8600e+03,  2.1660e+03, -7.8940e+03, -1.5070e+03, -1.4238e+04,\n",
              "       -5.2100e+03,  1.8400e+03, -2.7860e+03, -5.0120e+03, -3.2880e+03,\n",
              "       -1.8700e+03,  9.2390e+03, -6.5400e+02,  1.1780e+03, -3.1220e+03,\n",
              "       -1.0460e+03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "       -4.4110e+03, -2.4000e+02,  8.9500e+02,  2.9730e+03,  3.4160e+03,\n",
              "        3.5260e+03, -1.3070e+03, -1.9420e+03, -8.4900e+02,  4.7890e+03,\n",
              "       -3.8800e+02, -9.2090e+03, -1.3100e+03, -4.0470e+03, -6.3600e+02,\n",
              "       -1.5100e+02,  1.5080e+03,  5.0870e+03, -6.0000e+02, -1.5080e+03,\n",
              "        3.5190e+03, -9.4000e+01, -1.1170e+03, -7.6800e+02,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00, -1.4000e+01, -3.9350e+03, -2.5980e+03,\n",
              "        3.0940e+03,  8.6300e+02,  4.2470e+03,  5.5200e+02,  4.0220e+03,\n",
              "        4.7180e+03, -2.6190e+03,  2.7820e+03, -1.2355e+04, -7.6740e+03,\n",
              "        1.3800e+02, -4.9090e+03, -3.4190e+03,  6.3000e+01, -3.5900e+03,\n",
              "       -2.1830e+03,  1.3290e+03,  1.1830e+03, -1.1900e+02,  3.0750e+03,\n",
              "       -8.1300e+02, -1.5930e+03, -1.5900e+02,  0.0000e+00,  0.0000e+00,\n",
              "       -7.3700e+02, -4.6410e+03, -2.9530e+03, -2.0670e+03, -3.2490e+03,\n",
              "        2.5280e+03,  3.3070e+03,  4.4600e+03, -3.0690e+03, -1.2170e+03,\n",
              "        1.0910e+03, -5.6930e+03, -1.0523e+04, -3.4400e+02, -3.6800e+03,\n",
              "        3.0800e+02,  9.2900e+02,  1.2110e+03,  3.5210e+03,  4.8900e+02,\n",
              "       -2.9710e+03, -4.3060e+03, -1.0240e+03, -3.8700e+03, -2.3210e+03,\n",
              "       -2.3400e+02,  0.0000e+00,  0.0000e+00, -6.6900e+02, -2.5440e+03,\n",
              "        2.9050e+03,  4.7250e+03,  4.7960e+03, -1.4410e+03,  2.0000e+01,\n",
              "       -4.2930e+03,  2.0880e+03, -2.5070e+03, -1.5030e+03, -7.7520e+03,\n",
              "       -5.7120e+03, -1.6720e+03, -2.8180e+03,  1.0320e+03, -2.8700e+03,\n",
              "        2.0200e+02,  1.7010e+03, -1.8410e+03,  4.4990e+03,  2.2070e+03,\n",
              "       -1.3840e+03, -1.4790e+03, -1.2450e+03,  1.2470e+03,  0.0000e+00,\n",
              "        0.0000e+00, -2.2200e+02, -4.9870e+03, -4.6930e+03, -2.6140e+03,\n",
              "        6.6100e+02, -1.9160e+03,  1.8960e+03,  2.6820e+03,  4.3170e+03,\n",
              "       -2.3150e+03, -5.2200e+02, -9.1120e+03, -3.3130e+03,  2.0300e+02,\n",
              "       -3.3420e+03,  2.1890e+03, -2.2010e+03,  4.2600e+02, -4.8700e+03,\n",
              "       -3.1080e+03,  1.9460e+03,  1.2400e+02, -3.8120e+03, -3.0370e+03,\n",
              "       -1.8170e+03, -1.0340e+03,  0.0000e+00,  0.0000e+00, -3.2400e+02,\n",
              "       -8.5850e+03, -5.4000e+01, -3.9850e+03, -1.3420e+03, -2.0850e+03,\n",
              "        2.1500e+02, -1.9450e+03,  3.5000e+01,  7.9200e+02,  6.1710e+03,\n",
              "       -5.1760e+03, -1.4950e+03, -2.0420e+03, -1.1570e+03,  1.0180e+03,\n",
              "        1.3640e+03, -9.0600e+02, -1.6200e+03,  1.9250e+03, -1.3400e+03,\n",
              "       -4.9620e+03, -2.9470e+03, -2.5190e+03, -9.7600e+02, -1.2800e+02,\n",
              "        0.0000e+00,  0.0000e+00, -9.2200e+02, -9.0900e+03, -2.4000e+01,\n",
              "        2.5860e+03, -1.8730e+03,  8.6300e+02,  2.1300e+02, -2.5030e+03,\n",
              "        4.8710e+03,  3.9800e+02,  1.9860e+03,  1.8830e+03, -6.0480e+03,\n",
              "        4.3240e+03, -5.0500e+03, -1.4840e+03, -2.3390e+03, -1.6980e+03,\n",
              "        2.9530e+03, -2.9620e+03, -1.8060e+03, -2.4370e+03, -1.9410e+03,\n",
              "       -1.5370e+03, -1.0400e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "       -1.8730e+03, -1.8300e+03,  4.3100e+02, -8.2500e+02, -1.8580e+03,\n",
              "       -1.4650e+03, -3.8480e+03,  2.9030e+03,  2.7950e+03,  1.7520e+03,\n",
              "        1.7180e+03, -3.7910e+03,  2.4460e+03, -1.1380e+03,  1.8750e+03,\n",
              "       -4.8050e+03, -1.1360e+03, -4.1430e+03, -6.8430e+03, -4.1500e+02,\n",
              "        4.0000e+02, -4.5290e+03, -3.0560e+03, -1.2630e+03,  1.8620e+03,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00, -4.0200e+02, -1.4200e+02,\n",
              "       -6.5850e+03, -4.7300e+02,  3.9830e+03,  1.4700e+03,  2.9800e+02,\n",
              "        2.8410e+03,  3.0000e+00,  1.6040e+03, -1.8770e+03,  7.2440e+03,\n",
              "       -1.7530e+03, -3.1000e+02, -1.9670e+03,  3.1510e+03, -3.8340e+03,\n",
              "        8.8000e+01,  5.3320e+03,  2.7000e+01,  7.0300e+02, -1.0930e+03,\n",
              "       -5.4810e+03, -2.7950e+03,  3.1200e+02,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00, -7.1400e+02, -7.0950e+03, -4.1410e+03,\n",
              "        3.0200e+02,  1.0370e+03, -2.3220e+03,  1.2700e+02, -2.7300e+02,\n",
              "       -7.2000e+02,  7.0900e+02,  6.6500e+02,  2.0890e+03, -1.0150e+03,\n",
              "        1.1700e+02,  3.1980e+03, -3.9720e+03, -2.0560e+03, -8.0010e+03,\n",
              "       -5.2170e+03, -2.5650e+03, -1.5100e+03, -2.0280e+03, -1.8450e+03,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00, -1.1790e+03, -2.8720e+03, -2.7830e+03,  1.0040e+03,\n",
              "       -3.6700e+03, -1.1730e+03,  8.9700e+02, -1.3650e+03,  4.7700e+02,\n",
              "       -5.1760e+03,  3.7510e+03, -2.2120e+03, -6.3160e+03, -4.5420e+03,\n",
              "       -3.9070e+03, -3.1650e+03, -4.6610e+03, -4.3600e+03, -2.8690e+03,\n",
              "       -1.9520e+03, -1.3070e+03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.7700e+02,\n",
              "       -3.4730e+03, -7.1400e+03, -7.4440e+03, -5.5640e+03, -7.6460e+03,\n",
              "       -1.0704e+04, -8.6800e+03, -1.4412e+04, -1.0721e+04, -7.8710e+03,\n",
              "       -1.4474e+04, -1.0522e+04, -1.0506e+04, -9.2210e+03, -6.5940e+03,\n",
              "       -6.6230e+03, -3.9790e+03, -2.5170e+03, -1.1070e+03, -3.5000e+02,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00, -8.3000e+01, -1.9770e+03, -2.4220e+03,\n",
              "       -3.7510e+03, -3.2990e+03, -4.3740e+03, -5.7150e+03, -5.2550e+03,\n",
              "       -4.9570e+03, -6.8760e+03, -7.8140e+03, -6.5660e+03, -4.9130e+03,\n",
              "       -5.9100e+03, -4.3390e+03, -1.0280e+03, -2.2280e+03, -2.6450e+03,\n",
              "       -2.5450e+03, -4.4800e+02, -2.8000e+01,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.6000e+01,\n",
              "       -9.4700e+02, -1.0240e+03, -4.5700e+02, -3.7500e+02, -6.8800e+02,\n",
              "       -1.9610e+03, -3.6540e+03, -2.5150e+03, -1.5000e+01, -1.8300e+02,\n",
              "       -5.4000e+01,  0.0000e+00, -7.6100e+02, -2.7640e+03, -6.5000e+02,\n",
              "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "per.coef_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Mc37BXJl0oW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e04702-98fe-479e-9366-bd85ddb629a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 784)\n"
          ]
        }
      ],
      "source": [
        "print(per.coef_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "f_dOD5DI0ys2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94045784-5fe7-436d-ee3f-03c4146eb6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,)\n"
          ]
        }
      ],
      "source": [
        "print(per.intercept_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "p9-UkGY103J4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecb5369-e65e-48c8-97d4-52f193c78782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      3279\n",
            "           1       0.98      0.96      0.97      3760\n",
            "           2       0.92      0.88      0.90      3320\n",
            "           3       0.87      0.89      0.88      3472\n",
            "           4       0.97      0.83      0.89      3262\n",
            "           5       0.77      0.88      0.82      3051\n",
            "           6       0.96      0.94      0.95      3300\n",
            "           7       0.92      0.93      0.92      3543\n",
            "           8       0.86      0.77      0.81      3272\n",
            "           9       0.79      0.92      0.85      3341\n",
            "\n",
            "    accuracy                           0.90     33600\n",
            "   macro avg       0.90      0.90      0.90     33600\n",
            "weighted avg       0.90      0.90      0.90     33600\n",
            "\n",
            "[[3158    1    8   19    4   35   24    2   11   17]\n",
            " [   0 3622   13   33    2   28    3   19   26   14]\n",
            " [  10   19 2925  124   26   50   24   50   76   16]\n",
            " [   2    2   79 3100    2  123    6   29   82   47]\n",
            " [   3    8   24   12 2699   28   22   29   66  371]\n",
            " [  19    2   18  126   22 2670   19    9   86   80]\n",
            " [  14    6   19    6    3  112 3105    5   23    7]\n",
            " [  10    4   32   18   12    2    1 3305    7  152]\n",
            " [  10   38   58   93    5  392   15   33 2528  100]\n",
            " [   9    5   16   43   10   27    1  123   44 3063]]\n",
            "Accuracy Score: 0.8980654761904762\n",
            "\n",
            "Testing Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96       853\n",
            "           1       0.96      0.95      0.95       924\n",
            "           2       0.89      0.86      0.87       857\n",
            "           3       0.85      0.88      0.86       879\n",
            "           4       0.95      0.80      0.87       810\n",
            "           5       0.75      0.86      0.80       744\n",
            "           6       0.96      0.93      0.94       837\n",
            "           7       0.89      0.91      0.90       858\n",
            "           8       0.81      0.74      0.78       791\n",
            "           9       0.78      0.89      0.83       847\n",
            "\n",
            "    accuracy                           0.88      8400\n",
            "   macro avg       0.88      0.88      0.88      8400\n",
            "weighted avg       0.88      0.88      0.88      8400\n",
            "\n",
            "[[812   0   2   3   2  14   5   3   9   3]\n",
            " [  0 876   8  12   0   8   1   2  10   7]\n",
            " [  6   9 734  32   5  12   6  18  31   4]\n",
            " [  2   3  19 773   0  33   2   7  23  17]\n",
            " [  2   3  10   2 648   8   5  10  21 101]\n",
            " [  3   2   4  36   4 642   9   4  25  15]\n",
            " [  6   2  11   1   3  30 775   3   5   1]\n",
            " [  1   2  16   3   8   3   0 785   1  39]\n",
            " [  1  14  16  33   5 105   1   5 588  23]\n",
            " [  4   0   2  14   9   5   0  45  13 755]]\n",
            "Accuracy Score: 0.8795238095238095\n"
          ]
        }
      ],
      "source": [
        "model_evaluation(per, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSZx2_K792Np"
      },
      "source": [
        "MultiLayer Perceptron using TensorFlow\n",
        "* Tensor --> n d array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry8qKNg9GR6B"
      },
      "source": [
        "Approach to Keras Model\n",
        "1. Declare a Model\n",
        "   1. Sequential Model: Sequence of layers\n",
        "   2. Functional Model: Models with inter connected outputs\n",
        "2. Declare the layers\n",
        "   1. Type of Layer (Dense, Conv Layer, RNN, LSTM....)\n",
        "   2. Number of neurons (32, 64, 128, 256, 512)\n",
        "   3. Activation function (Relu, Sigmoid, Softmax, Tanh)\n",
        "3. Compile the layers\n",
        "   1. Loss Function (Binary Cross Entropy, Categorical Cross Entropy...)\n",
        "   2. Optimizer (SGD, Adam, AdaGrad....)\n",
        "   3. Metrics (Accuracy Score, Precision, Recall)\n",
        "4. Fit the model\n",
        "   1. input and output (x_train, y_train)\n",
        "   2. Number of Iterations (20, 40, 50, 100, 200)\n",
        "   3. Validation dataset (x_test, y_test)\n",
        "5. Model Evaluation\n",
        "   1. Classification: Precision, Recall, Classification Report\n",
        "   2. Regression: MSE, RMSE, R2 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CB_hswjM1i-a"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdXaX5wRHJ9r"
      },
      "source": [
        "1. Declare the model and layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "isWj1oYpA1i3"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "#Declare the input Layer\n",
        "model.add(Dense(64,activation=\"relu\",input_dim=(784)))\n",
        "# Hiddin Layer 1\n",
        "model.add(Dense(64,activation=\"relu\"))\n",
        "# hidden layer 2\n",
        "model.add(Dense(64,activation=\"relu\"))\n",
        "# output layer\n",
        "model.add(Dense(10,activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahTHjpDQCa9l"
      },
      "source": [
        "Compile the layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSHD7EhQJ-U0"
      },
      "outputs": [],
      "source": [
        "#model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "XgvIxfeAPI3g"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Uf8l9Ap5O1eb"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqjJ7Cf8_kA_",
        "outputId": "e21e862d-6737-472f-dd84-1732a01cf356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6757 - loss: 4.4729 - val_accuracy: 0.8826 - val_loss: 0.4899\n",
            "Epoch 2/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8944 - loss: 0.4059 - val_accuracy: 0.9037 - val_loss: 0.3481\n",
            "Epoch 3/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9194 - loss: 0.2955 - val_accuracy: 0.9212 - val_loss: 0.2923\n",
            "Epoch 4/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9337 - loss: 0.2466 - val_accuracy: 0.9271 - val_loss: 0.2793\n",
            "Epoch 5/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9412 - loss: 0.2137 - val_accuracy: 0.9236 - val_loss: 0.2781\n",
            "Epoch 6/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9482 - loss: 0.1792 - val_accuracy: 0.9411 - val_loss: 0.2112\n",
            "Epoch 7/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9532 - loss: 0.1640 - val_accuracy: 0.9444 - val_loss: 0.2045\n",
            "Epoch 8/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9587 - loss: 0.1407 - val_accuracy: 0.9477 - val_loss: 0.1893\n",
            "Epoch 9/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9646 - loss: 0.1282 - val_accuracy: 0.9506 - val_loss: 0.1864\n",
            "Epoch 10/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.1172 - val_accuracy: 0.9535 - val_loss: 0.1721\n",
            "Epoch 11/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.1087 - val_accuracy: 0.9533 - val_loss: 0.1803\n",
            "Epoch 12/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9713 - loss: 0.1011 - val_accuracy: 0.9499 - val_loss: 0.2026\n",
            "Epoch 13/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.0969 - val_accuracy: 0.9543 - val_loss: 0.1827\n",
            "Epoch 14/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.0838 - val_accuracy: 0.9563 - val_loss: 0.1703\n",
            "Epoch 15/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0784 - val_accuracy: 0.9560 - val_loss: 0.1746\n",
            "Epoch 16/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0748 - val_accuracy: 0.9557 - val_loss: 0.2008\n",
            "Epoch 17/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9769 - loss: 0.0777 - val_accuracy: 0.9575 - val_loss: 0.1821\n",
            "Epoch 18/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9770 - loss: 0.0736 - val_accuracy: 0.9536 - val_loss: 0.2139\n",
            "Epoch 19/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9793 - loss: 0.0692 - val_accuracy: 0.9574 - val_loss: 0.1902\n",
            "Epoch 20/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.0630 - val_accuracy: 0.9529 - val_loss: 0.1947\n",
            "Epoch 21/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.0595 - val_accuracy: 0.9557 - val_loss: 0.1927\n",
            "Epoch 22/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9828 - loss: 0.0566 - val_accuracy: 0.9601 - val_loss: 0.1895\n",
            "Epoch 23/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9848 - loss: 0.0516 - val_accuracy: 0.9513 - val_loss: 0.2106\n",
            "Epoch 24/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9834 - loss: 0.0598 - val_accuracy: 0.9524 - val_loss: 0.2577\n",
            "Epoch 25/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9825 - loss: 0.0612 - val_accuracy: 0.9590 - val_loss: 0.2116\n",
            "Epoch 26/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9856 - loss: 0.0447 - val_accuracy: 0.9575 - val_loss: 0.2114\n",
            "Epoch 27/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9845 - loss: 0.0563 - val_accuracy: 0.9551 - val_loss: 0.2481\n",
            "Epoch 28/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9838 - loss: 0.0568 - val_accuracy: 0.9575 - val_loss: 0.2298\n",
            "Epoch 29/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9851 - loss: 0.0523 - val_accuracy: 0.9604 - val_loss: 0.2032\n",
            "Epoch 30/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9864 - loss: 0.0509 - val_accuracy: 0.9598 - val_loss: 0.2036\n",
            "Epoch 31/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0442 - val_accuracy: 0.9560 - val_loss: 0.2343\n",
            "Epoch 32/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0439 - val_accuracy: 0.9565 - val_loss: 0.2573\n",
            "Epoch 33/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.0458 - val_accuracy: 0.9583 - val_loss: 0.2319\n",
            "Epoch 34/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0447 - val_accuracy: 0.9604 - val_loss: 0.2211\n",
            "Epoch 35/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0376 - val_accuracy: 0.9594 - val_loss: 0.2187\n",
            "Epoch 36/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0387 - val_accuracy: 0.9568 - val_loss: 0.2338\n",
            "Epoch 37/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0421 - val_accuracy: 0.9592 - val_loss: 0.2286\n",
            "Epoch 38/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9878 - loss: 0.0459 - val_accuracy: 0.9586 - val_loss: 0.2289\n",
            "Epoch 39/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9881 - loss: 0.0424 - val_accuracy: 0.9542 - val_loss: 0.2403\n",
            "Epoch 40/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9913 - loss: 0.0293 - val_accuracy: 0.9577 - val_loss: 0.2559\n",
            "Epoch 41/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0463 - val_accuracy: 0.9630 - val_loss: 0.2543\n",
            "Epoch 42/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0349 - val_accuracy: 0.9592 - val_loss: 0.2770\n",
            "Epoch 43/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0404 - val_accuracy: 0.9624 - val_loss: 0.2284\n",
            "Epoch 44/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0306 - val_accuracy: 0.9576 - val_loss: 0.2852\n",
            "Epoch 45/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.0339 - val_accuracy: 0.9618 - val_loss: 0.2306\n",
            "Epoch 46/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9903 - loss: 0.0361 - val_accuracy: 0.9545 - val_loss: 0.2873\n",
            "Epoch 47/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9904 - loss: 0.0320 - val_accuracy: 0.9587 - val_loss: 0.2837\n",
            "Epoch 48/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0383 - val_accuracy: 0.9575 - val_loss: 0.2940\n",
            "Epoch 49/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9903 - loss: 0.0341 - val_accuracy: 0.9637 - val_loss: 0.2424\n",
            "Epoch 50/50\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9922 - loss: 0.0291 - val_accuracy: 0.9583 - val_loss: 0.3120\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(x_train, y_train, epochs = 50, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Layer Neural Network"
      ],
      "metadata": {
        "id": "2obe4da3_VMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6m2ziXcV_j64"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "#Declare the input Layer\n",
        "model.add(Dense(250,activation=\"relu\",input_dim=(784)))\n",
        "# Hiddin Layer 1\n",
        "model.add(Dense(250,activation=\"relu\"))\n",
        "# hidden layer 2\n",
        "model.add(Dense(250,activation=\"relu\"))\n",
        "# output layer\n",
        "model.add(Dense(10,activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OpuIGfga_j38"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lyts8lj_j05"
      },
      "outputs": [],
      "source": [
        "# y_train = to_categorical(y_train)\n",
        "# y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x_train, y_train, epochs = 100, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR7fHS7q_woP",
        "outputId": "4de33c09-2d3e-440c-bd82-254dd5f9e0da"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.8038 - loss: 3.9386 - val_accuracy: 0.9204 - val_loss: 0.3365\n",
            "Epoch 2/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9355 - loss: 0.2586 - val_accuracy: 0.9321 - val_loss: 0.2591\n",
            "Epoch 3/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9525 - loss: 0.1701 - val_accuracy: 0.9467 - val_loss: 0.2238\n",
            "Epoch 4/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9579 - loss: 0.1477 - val_accuracy: 0.9563 - val_loss: 0.1814\n",
            "Epoch 5/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9642 - loss: 0.1311 - val_accuracy: 0.9544 - val_loss: 0.1935\n",
            "Epoch 6/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9695 - loss: 0.1071 - val_accuracy: 0.9600 - val_loss: 0.1548\n",
            "Epoch 7/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9731 - loss: 0.0975 - val_accuracy: 0.9515 - val_loss: 0.1981\n",
            "Epoch 8/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9713 - loss: 0.1058 - val_accuracy: 0.9594 - val_loss: 0.1781\n",
            "Epoch 9/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9745 - loss: 0.0937 - val_accuracy: 0.9544 - val_loss: 0.1796\n",
            "Epoch 10/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9753 - loss: 0.0931 - val_accuracy: 0.9562 - val_loss: 0.1783\n",
            "Epoch 11/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9778 - loss: 0.0809 - val_accuracy: 0.9569 - val_loss: 0.1920\n",
            "Epoch 12/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9801 - loss: 0.0715 - val_accuracy: 0.9651 - val_loss: 0.1718\n",
            "Epoch 13/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9818 - loss: 0.0718 - val_accuracy: 0.9673 - val_loss: 0.1598\n",
            "Epoch 14/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9830 - loss: 0.0630 - val_accuracy: 0.9679 - val_loss: 0.1833\n",
            "Epoch 15/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9859 - loss: 0.0551 - val_accuracy: 0.9642 - val_loss: 0.1874\n",
            "Epoch 16/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9842 - loss: 0.0637 - val_accuracy: 0.9720 - val_loss: 0.1469\n",
            "Epoch 17/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9887 - loss: 0.0478 - val_accuracy: 0.9689 - val_loss: 0.1868\n",
            "Epoch 18/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9887 - loss: 0.0462 - val_accuracy: 0.9615 - val_loss: 0.2073\n",
            "Epoch 19/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9868 - loss: 0.0631 - val_accuracy: 0.9543 - val_loss: 0.2764\n",
            "Epoch 20/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9860 - loss: 0.0612 - val_accuracy: 0.9711 - val_loss: 0.1639\n",
            "Epoch 21/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9911 - loss: 0.0392 - val_accuracy: 0.9682 - val_loss: 0.1708\n",
            "Epoch 22/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9926 - loss: 0.0303 - val_accuracy: 0.9682 - val_loss: 0.1751\n",
            "Epoch 23/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9881 - loss: 0.0588 - val_accuracy: 0.9712 - val_loss: 0.2006\n",
            "Epoch 24/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9927 - loss: 0.0324 - val_accuracy: 0.9705 - val_loss: 0.2398\n",
            "Epoch 25/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9914 - loss: 0.0448 - val_accuracy: 0.9608 - val_loss: 0.2094\n",
            "Epoch 26/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9923 - loss: 0.0303 - val_accuracy: 0.9675 - val_loss: 0.1930\n",
            "Epoch 27/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9929 - loss: 0.0318 - val_accuracy: 0.9664 - val_loss: 0.2022\n",
            "Epoch 28/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9898 - loss: 0.0436 - val_accuracy: 0.9729 - val_loss: 0.1853\n",
            "Epoch 29/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9958 - loss: 0.0158 - val_accuracy: 0.9731 - val_loss: 0.2090\n",
            "Epoch 30/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.0404 - val_accuracy: 0.9686 - val_loss: 0.2904\n",
            "Epoch 31/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9918 - loss: 0.0456 - val_accuracy: 0.9713 - val_loss: 0.2072\n",
            "Epoch 32/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9939 - loss: 0.0293 - val_accuracy: 0.9592 - val_loss: 0.2554\n",
            "Epoch 33/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9896 - loss: 0.0538 - val_accuracy: 0.9763 - val_loss: 0.1828\n",
            "Epoch 34/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9963 - loss: 0.0213 - val_accuracy: 0.9726 - val_loss: 0.2287\n",
            "Epoch 35/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9922 - loss: 0.0491 - val_accuracy: 0.9779 - val_loss: 0.1828\n",
            "Epoch 36/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9953 - loss: 0.0227 - val_accuracy: 0.9751 - val_loss: 0.2215\n",
            "Epoch 37/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9944 - loss: 0.0441 - val_accuracy: 0.9738 - val_loss: 0.2361\n",
            "Epoch 38/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9971 - loss: 0.0135 - val_accuracy: 0.9731 - val_loss: 0.3196\n",
            "Epoch 39/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9937 - loss: 0.0347 - val_accuracy: 0.9693 - val_loss: 0.3163\n",
            "Epoch 40/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9927 - loss: 0.0461 - val_accuracy: 0.9714 - val_loss: 0.2717\n",
            "Epoch 41/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9942 - loss: 0.0307 - val_accuracy: 0.9736 - val_loss: 0.2690\n",
            "Epoch 42/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9944 - loss: 0.0311 - val_accuracy: 0.9727 - val_loss: 0.2882\n",
            "Epoch 43/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9951 - loss: 0.0265 - val_accuracy: 0.9757 - val_loss: 0.2624\n",
            "Epoch 44/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9955 - loss: 0.0253 - val_accuracy: 0.9743 - val_loss: 0.2777\n",
            "Epoch 45/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0439 - val_accuracy: 0.9706 - val_loss: 0.3140\n",
            "Epoch 46/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9921 - loss: 0.0526 - val_accuracy: 0.9658 - val_loss: 0.3720\n",
            "Epoch 47/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9956 - loss: 0.0294 - val_accuracy: 0.9725 - val_loss: 0.2800\n",
            "Epoch 48/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0168 - val_accuracy: 0.9762 - val_loss: 0.3274\n",
            "Epoch 49/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9953 - loss: 0.0381 - val_accuracy: 0.9688 - val_loss: 0.3489\n",
            "Epoch 50/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9942 - loss: 0.0352 - val_accuracy: 0.9724 - val_loss: 0.3338\n",
            "Epoch 51/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9960 - loss: 0.0225 - val_accuracy: 0.9706 - val_loss: 0.3349\n",
            "Epoch 52/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9938 - loss: 0.0476 - val_accuracy: 0.9714 - val_loss: 0.4008\n",
            "Epoch 53/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9951 - loss: 0.0246 - val_accuracy: 0.9744 - val_loss: 0.3446\n",
            "Epoch 54/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9962 - loss: 0.0315 - val_accuracy: 0.9646 - val_loss: 0.3556\n",
            "Epoch 55/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9927 - loss: 0.0348 - val_accuracy: 0.9704 - val_loss: 0.2557\n",
            "Epoch 56/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9932 - loss: 0.0380 - val_accuracy: 0.9698 - val_loss: 0.3544\n",
            "Epoch 57/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9948 - loss: 0.0285 - val_accuracy: 0.9663 - val_loss: 0.3450\n",
            "Epoch 58/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9932 - loss: 0.0442 - val_accuracy: 0.9745 - val_loss: 0.2994\n",
            "Epoch 59/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9963 - loss: 0.0358 - val_accuracy: 0.9739 - val_loss: 0.3180\n",
            "Epoch 60/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9955 - loss: 0.0265 - val_accuracy: 0.9738 - val_loss: 0.5118\n",
            "Epoch 61/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9964 - loss: 0.0248 - val_accuracy: 0.9698 - val_loss: 0.4999\n",
            "Epoch 62/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9942 - loss: 0.0390 - val_accuracy: 0.9681 - val_loss: 0.6330\n",
            "Epoch 63/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9954 - loss: 0.0360 - val_accuracy: 0.9692 - val_loss: 0.3866\n",
            "Epoch 64/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9957 - loss: 0.0223 - val_accuracy: 0.9737 - val_loss: 0.4135\n",
            "Epoch 65/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9941 - loss: 0.0369 - val_accuracy: 0.9667 - val_loss: 0.6059\n",
            "Epoch 66/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9938 - loss: 0.0494 - val_accuracy: 0.9677 - val_loss: 0.4477\n",
            "Epoch 67/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9958 - loss: 0.0390 - val_accuracy: 0.9742 - val_loss: 0.5390\n",
            "Epoch 68/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9949 - loss: 0.0334 - val_accuracy: 0.9670 - val_loss: 0.3702\n",
            "Epoch 69/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9930 - loss: 0.0535 - val_accuracy: 0.9673 - val_loss: 0.4610\n",
            "Epoch 70/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9936 - loss: 0.0358 - val_accuracy: 0.9731 - val_loss: 0.4456\n",
            "Epoch 71/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9944 - loss: 0.0365 - val_accuracy: 0.9699 - val_loss: 0.4240\n",
            "Epoch 72/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9945 - loss: 0.0406 - val_accuracy: 0.9738 - val_loss: 0.4868\n",
            "Epoch 73/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9927 - loss: 0.0498 - val_accuracy: 0.9690 - val_loss: 0.3378\n",
            "Epoch 74/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.9948 - loss: 0.0307 - val_accuracy: 0.9736 - val_loss: 0.4290\n",
            "Epoch 75/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0280 - val_accuracy: 0.9608 - val_loss: 0.3843\n",
            "Epoch 76/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9940 - loss: 0.0317 - val_accuracy: 0.9737 - val_loss: 0.3923\n",
            "Epoch 77/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9956 - loss: 0.0269 - val_accuracy: 0.9738 - val_loss: 0.3765\n",
            "Epoch 78/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9960 - loss: 0.0209 - val_accuracy: 0.9686 - val_loss: 0.3913\n",
            "Epoch 79/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9955 - loss: 0.0270 - val_accuracy: 0.9625 - val_loss: 0.8361\n",
            "Epoch 80/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9916 - loss: 0.0714 - val_accuracy: 0.9706 - val_loss: 0.5552\n",
            "Epoch 81/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9938 - loss: 0.0474 - val_accuracy: 0.9751 - val_loss: 0.6081\n",
            "Epoch 82/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0289 - val_accuracy: 0.9627 - val_loss: 0.5700\n",
            "Epoch 83/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9945 - loss: 0.0537 - val_accuracy: 0.9760 - val_loss: 0.5618\n",
            "Epoch 84/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9947 - loss: 0.0422 - val_accuracy: 0.9704 - val_loss: 0.5774\n",
            "Epoch 85/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0239 - val_accuracy: 0.9718 - val_loss: 0.5663\n",
            "Epoch 86/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9934 - loss: 0.0320 - val_accuracy: 0.9644 - val_loss: 0.6530\n",
            "Epoch 87/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9896 - loss: 0.0579 - val_accuracy: 0.9637 - val_loss: 0.4215\n",
            "Epoch 88/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9914 - loss: 0.0308 - val_accuracy: 0.9671 - val_loss: 0.3448\n",
            "Epoch 89/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9948 - loss: 0.0214 - val_accuracy: 0.9745 - val_loss: 0.4297\n",
            "Epoch 90/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9972 - loss: 0.0161 - val_accuracy: 0.9724 - val_loss: 0.4718\n",
            "Epoch 91/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9948 - loss: 0.0373 - val_accuracy: 0.9675 - val_loss: 0.7260\n",
            "Epoch 92/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9916 - loss: 0.0498 - val_accuracy: 0.9719 - val_loss: 0.6789\n",
            "Epoch 93/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0401 - val_accuracy: 0.9673 - val_loss: 0.5454\n",
            "Epoch 94/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9908 - loss: 0.0492 - val_accuracy: 0.9715 - val_loss: 0.5689\n",
            "Epoch 95/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9950 - loss: 0.0384 - val_accuracy: 0.9739 - val_loss: 0.6728\n",
            "Epoch 96/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9968 - loss: 0.0200 - val_accuracy: 0.9620 - val_loss: 0.7501\n",
            "Epoch 97/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9906 - loss: 0.0543 - val_accuracy: 0.9685 - val_loss: 0.5337\n",
            "Epoch 98/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9936 - loss: 0.0418 - val_accuracy: 0.9673 - val_loss: 1.0227\n",
            "Epoch 99/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9946 - loss: 0.0232 - val_accuracy: 0.9631 - val_loss: 1.0189\n",
            "Epoch 100/100\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9926 - loss: 0.0508 - val_accuracy: 0.9705 - val_loss: 0.8102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion here in this notebook we studies Machine learning random forest algorithum , Linear Perceptron and Decide and study multilayer neural network with different activation function"
      ],
      "metadata": {
        "id": "ehhXl04GcZ7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x_train, y_train, epochs = 150, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "3tr78Am5c5P-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c101c5d4-e8af-4c6c-ec69-04f2c769c62e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9936 - loss: 0.0460 - val_accuracy: 0.9676 - val_loss: 0.6473\n",
            "Epoch 2/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9957 - loss: 0.0287 - val_accuracy: 0.9700 - val_loss: 0.8840\n",
            "Epoch 3/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9957 - loss: 0.0225 - val_accuracy: 0.9664 - val_loss: 0.7317\n",
            "Epoch 4/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9920 - loss: 0.0510 - val_accuracy: 0.9700 - val_loss: 0.7362\n",
            "Epoch 5/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9849 - loss: 0.0725 - val_accuracy: 0.9662 - val_loss: 0.6899\n",
            "Epoch 6/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9929 - loss: 0.0482 - val_accuracy: 0.9632 - val_loss: 0.6131\n",
            "Epoch 7/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9916 - loss: 0.0458 - val_accuracy: 0.9675 - val_loss: 0.9881\n",
            "Epoch 8/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9913 - loss: 0.0493 - val_accuracy: 0.9613 - val_loss: 0.6678\n",
            "Epoch 9/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9936 - loss: 0.0266 - val_accuracy: 0.9698 - val_loss: 1.0572\n",
            "Epoch 10/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9933 - loss: 0.0407 - val_accuracy: 0.9607 - val_loss: 0.9530\n",
            "Epoch 11/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9913 - loss: 0.0464 - val_accuracy: 0.9586 - val_loss: 0.7857\n",
            "Epoch 12/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9916 - loss: 0.0723 - val_accuracy: 0.9707 - val_loss: 1.1458\n",
            "Epoch 13/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9937 - loss: 0.0420 - val_accuracy: 0.9654 - val_loss: 1.4672\n",
            "Epoch 14/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9931 - loss: 0.0622 - val_accuracy: 0.9705 - val_loss: 0.7796\n",
            "Epoch 15/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9943 - loss: 0.0252 - val_accuracy: 0.9623 - val_loss: 0.8599\n",
            "Epoch 16/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9905 - loss: 0.0542 - val_accuracy: 0.9620 - val_loss: 0.7154\n",
            "Epoch 17/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9910 - loss: 0.0342 - val_accuracy: 0.9667 - val_loss: 1.2565\n",
            "Epoch 18/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9934 - loss: 0.0484 - val_accuracy: 0.9683 - val_loss: 1.0925\n",
            "Epoch 19/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9914 - loss: 0.0882 - val_accuracy: 0.9715 - val_loss: 0.5610\n",
            "Epoch 20/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9937 - loss: 0.0275 - val_accuracy: 0.9635 - val_loss: 1.0535\n",
            "Epoch 21/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9889 - loss: 0.0884 - val_accuracy: 0.9665 - val_loss: 0.9429\n",
            "Epoch 22/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9895 - loss: 0.0627 - val_accuracy: 0.9679 - val_loss: 0.8596\n",
            "Epoch 23/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9930 - loss: 0.0447 - val_accuracy: 0.9675 - val_loss: 1.0046\n",
            "Epoch 24/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9910 - loss: 0.0585 - val_accuracy: 0.9639 - val_loss: 0.6062\n",
            "Epoch 25/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.0266 - val_accuracy: 0.9679 - val_loss: 0.9817\n",
            "Epoch 26/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9877 - loss: 0.1021 - val_accuracy: 0.9707 - val_loss: 0.8287\n",
            "Epoch 27/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9885 - loss: 0.0806 - val_accuracy: 0.9637 - val_loss: 0.8452\n",
            "Epoch 28/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9933 - loss: 0.0427 - val_accuracy: 0.9725 - val_loss: 1.1525\n",
            "Epoch 29/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9914 - loss: 0.1404 - val_accuracy: 0.9660 - val_loss: 1.4416\n",
            "Epoch 30/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9898 - loss: 0.0515 - val_accuracy: 0.9631 - val_loss: 0.6457\n",
            "Epoch 31/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9904 - loss: 0.0485 - val_accuracy: 0.9725 - val_loss: 0.9874\n",
            "Epoch 32/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9919 - loss: 0.0718 - val_accuracy: 0.9644 - val_loss: 0.9609\n",
            "Epoch 33/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9904 - loss: 0.0501 - val_accuracy: 0.9493 - val_loss: 1.1736\n",
            "Epoch 34/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9871 - loss: 0.0659 - val_accuracy: 0.9689 - val_loss: 0.7985\n",
            "Epoch 35/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9912 - loss: 0.0645 - val_accuracy: 0.9645 - val_loss: 0.7086\n",
            "Epoch 36/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9869 - loss: 0.0842 - val_accuracy: 0.9621 - val_loss: 1.0603\n",
            "Epoch 37/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9898 - loss: 0.0851 - val_accuracy: 0.9639 - val_loss: 0.8592\n",
            "Epoch 38/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9876 - loss: 0.0733 - val_accuracy: 0.9724 - val_loss: 1.3361\n",
            "Epoch 39/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9871 - loss: 0.1136 - val_accuracy: 0.9482 - val_loss: 0.7000\n",
            "Epoch 40/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9800 - loss: 0.0713 - val_accuracy: 0.9635 - val_loss: 0.9068\n",
            "Epoch 41/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9845 - loss: 0.0802 - val_accuracy: 0.9575 - val_loss: 0.8006\n",
            "Epoch 42/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9853 - loss: 0.0671 - val_accuracy: 0.9673 - val_loss: 1.5530\n",
            "Epoch 43/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9902 - loss: 0.0613 - val_accuracy: 0.9657 - val_loss: 1.3996\n",
            "Epoch 44/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9870 - loss: 0.0727 - val_accuracy: 0.9552 - val_loss: 1.2433\n",
            "Epoch 45/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9898 - loss: 0.0668 - val_accuracy: 0.9615 - val_loss: 0.9513\n",
            "Epoch 46/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9826 - loss: 0.0817 - val_accuracy: 0.9654 - val_loss: 1.2473\n",
            "Epoch 47/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9847 - loss: 0.0723 - val_accuracy: 0.9667 - val_loss: 1.3798\n",
            "Epoch 48/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9914 - loss: 0.0340 - val_accuracy: 0.9651 - val_loss: 1.2844\n",
            "Epoch 49/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9785 - loss: 0.1843 - val_accuracy: 0.9615 - val_loss: 1.9109\n",
            "Epoch 50/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9862 - loss: 0.0505 - val_accuracy: 0.9619 - val_loss: 1.5201\n",
            "Epoch 51/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9817 - loss: 0.0960 - val_accuracy: 0.9526 - val_loss: 2.1780\n",
            "Epoch 52/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9857 - loss: 0.0687 - val_accuracy: 0.9568 - val_loss: 1.2711\n",
            "Epoch 53/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9848 - loss: 0.0613 - val_accuracy: 0.9618 - val_loss: 1.6369\n",
            "Epoch 54/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9877 - loss: 0.0622 - val_accuracy: 0.9540 - val_loss: 2.7435\n",
            "Epoch 55/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9847 - loss: 0.1534 - val_accuracy: 0.9613 - val_loss: 1.7315\n",
            "Epoch 56/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9865 - loss: 0.0469 - val_accuracy: 0.9506 - val_loss: 1.5810\n",
            "Epoch 57/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9778 - loss: 0.0938 - val_accuracy: 0.9617 - val_loss: 1.6301\n",
            "Epoch 58/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9791 - loss: 0.1364 - val_accuracy: 0.9493 - val_loss: 1.5673\n",
            "Epoch 59/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9794 - loss: 0.1249 - val_accuracy: 0.9575 - val_loss: 1.6784\n",
            "Epoch 60/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9758 - loss: 0.2081 - val_accuracy: 0.9604 - val_loss: 1.3472\n",
            "Epoch 61/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9816 - loss: 0.1180 - val_accuracy: 0.9501 - val_loss: 1.7425\n",
            "Epoch 62/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9818 - loss: 0.0822 - val_accuracy: 0.9586 - val_loss: 1.4660\n",
            "Epoch 63/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9867 - loss: 0.0667 - val_accuracy: 0.9662 - val_loss: 1.2506\n",
            "Epoch 64/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9860 - loss: 0.0689 - val_accuracy: 0.9523 - val_loss: 1.7551\n",
            "Epoch 65/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9791 - loss: 0.1063 - val_accuracy: 0.9493 - val_loss: 1.4338\n",
            "Epoch 66/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9843 - loss: 0.0509 - val_accuracy: 0.9631 - val_loss: 2.0713\n",
            "Epoch 67/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9904 - loss: 0.1070 - val_accuracy: 0.9582 - val_loss: 2.0611\n",
            "Epoch 68/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9831 - loss: 0.0915 - val_accuracy: 0.9650 - val_loss: 1.9651\n",
            "Epoch 69/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9883 - loss: 0.0483 - val_accuracy: 0.9618 - val_loss: 1.1688\n",
            "Epoch 70/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9913 - loss: 0.0360 - val_accuracy: 0.9575 - val_loss: 1.4835\n",
            "Epoch 71/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9827 - loss: 0.1157 - val_accuracy: 0.9445 - val_loss: 1.0805\n",
            "Epoch 72/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9808 - loss: 0.0849 - val_accuracy: 0.9531 - val_loss: 1.2130\n",
            "Epoch 73/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9858 - loss: 0.0783 - val_accuracy: 0.9602 - val_loss: 1.2111\n",
            "Epoch 74/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9857 - loss: 0.0742 - val_accuracy: 0.9643 - val_loss: 1.5542\n",
            "Epoch 75/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9843 - loss: 0.0822 - val_accuracy: 0.9575 - val_loss: 1.6499\n",
            "Epoch 76/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9831 - loss: 0.1175 - val_accuracy: 0.9593 - val_loss: 1.5394\n",
            "Epoch 77/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9811 - loss: 0.0902 - val_accuracy: 0.9274 - val_loss: 1.0439\n",
            "Epoch 78/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9668 - loss: 0.1260 - val_accuracy: 0.9517 - val_loss: 1.6116\n",
            "Epoch 79/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9786 - loss: 0.1117 - val_accuracy: 0.9546 - val_loss: 1.2080\n",
            "Epoch 80/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9757 - loss: 0.0947 - val_accuracy: 0.9630 - val_loss: 2.4542\n",
            "Epoch 81/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9790 - loss: 0.2171 - val_accuracy: 0.9374 - val_loss: 1.2964\n",
            "Epoch 82/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9708 - loss: 0.1202 - val_accuracy: 0.9544 - val_loss: 1.3002\n",
            "Epoch 83/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9792 - loss: 0.0611 - val_accuracy: 0.9557 - val_loss: 1.2376\n",
            "Epoch 84/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9860 - loss: 0.0603 - val_accuracy: 0.9554 - val_loss: 1.3660\n",
            "Epoch 85/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9847 - loss: 0.0831 - val_accuracy: 0.9429 - val_loss: 1.1012\n",
            "Epoch 86/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9787 - loss: 0.0741 - val_accuracy: 0.9630 - val_loss: 2.2321\n",
            "Epoch 87/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9903 - loss: 0.0423 - val_accuracy: 0.9546 - val_loss: 3.1388\n",
            "Epoch 88/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9777 - loss: 0.2374 - val_accuracy: 0.9645 - val_loss: 2.0604\n",
            "Epoch 89/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9855 - loss: 0.0623 - val_accuracy: 0.9474 - val_loss: 1.9707\n",
            "Epoch 90/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9670 - loss: 0.1354 - val_accuracy: 0.9526 - val_loss: 2.6629\n",
            "Epoch 91/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9805 - loss: 0.0857 - val_accuracy: 0.9526 - val_loss: 1.6577\n",
            "Epoch 92/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9825 - loss: 0.0846 - val_accuracy: 0.9517 - val_loss: 2.2276\n",
            "Epoch 93/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9724 - loss: 0.1131 - val_accuracy: 0.9562 - val_loss: 2.1416\n",
            "Epoch 94/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9758 - loss: 0.1268 - val_accuracy: 0.9600 - val_loss: 2.8664\n",
            "Epoch 95/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9789 - loss: 0.1224 - val_accuracy: 0.9499 - val_loss: 1.8570\n",
            "Epoch 96/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9783 - loss: 0.0990 - val_accuracy: 0.9446 - val_loss: 1.2678\n",
            "Epoch 97/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9779 - loss: 0.0934 - val_accuracy: 0.9630 - val_loss: 3.6341\n",
            "Epoch 98/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9798 - loss: 0.1272 - val_accuracy: 0.9518 - val_loss: 2.7116\n",
            "Epoch 99/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9823 - loss: 0.0730 - val_accuracy: 0.9546 - val_loss: 1.9580\n",
            "Epoch 100/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9843 - loss: 0.0550 - val_accuracy: 0.9446 - val_loss: 2.4570\n",
            "Epoch 101/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9741 - loss: 0.1217 - val_accuracy: 0.9540 - val_loss: 2.0702\n",
            "Epoch 102/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9844 - loss: 0.1035 - val_accuracy: 0.9552 - val_loss: 2.3556\n",
            "Epoch 103/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9845 - loss: 0.0617 - val_accuracy: 0.9593 - val_loss: 2.6900\n",
            "Epoch 104/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9852 - loss: 0.0766 - val_accuracy: 0.9561 - val_loss: 2.2964\n",
            "Epoch 105/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9875 - loss: 0.0550 - val_accuracy: 0.9549 - val_loss: 2.1577\n",
            "Epoch 106/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9837 - loss: 0.0599 - val_accuracy: 0.9458 - val_loss: 1.2977\n",
            "Epoch 107/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9786 - loss: 0.0821 - val_accuracy: 0.9656 - val_loss: 2.5503\n",
            "Epoch 108/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9879 - loss: 0.0511 - val_accuracy: 0.9574 - val_loss: 1.6170\n",
            "Epoch 109/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9861 - loss: 0.0631 - val_accuracy: 0.9329 - val_loss: 2.1223\n",
            "Epoch 110/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9635 - loss: 0.1353 - val_accuracy: 0.9451 - val_loss: 1.7928\n",
            "Epoch 111/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9765 - loss: 0.0882 - val_accuracy: 0.9410 - val_loss: 2.2877\n",
            "Epoch 112/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.9747 - loss: 0.0774 - val_accuracy: 0.9519 - val_loss: 1.6613\n",
            "Epoch 113/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.9813 - loss: 0.0815 - val_accuracy: 0.9449 - val_loss: 2.3117\n",
            "Epoch 114/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9802 - loss: 0.0784 - val_accuracy: 0.9543 - val_loss: 2.6432\n",
            "Epoch 115/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9705 - loss: 0.1251 - val_accuracy: 0.9590 - val_loss: 2.6420\n",
            "Epoch 116/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9788 - loss: 0.1569 - val_accuracy: 0.9440 - val_loss: 2.4948\n",
            "Epoch 117/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.9753 - loss: 0.0912 - val_accuracy: 0.9501 - val_loss: 2.7554\n",
            "Epoch 118/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9770 - loss: 0.0811 - val_accuracy: 0.9440 - val_loss: 1.9673\n",
            "Epoch 119/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9750 - loss: 0.0831 - val_accuracy: 0.9589 - val_loss: 3.6212\n",
            "Epoch 120/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9797 - loss: 0.1011 - val_accuracy: 0.9571 - val_loss: 3.0936\n",
            "Epoch 121/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.9827 - loss: 0.1169 - val_accuracy: 0.9590 - val_loss: 2.4996\n",
            "Epoch 122/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.9839 - loss: 0.0771 - val_accuracy: 0.9490 - val_loss: 2.7220\n",
            "Epoch 123/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9742 - loss: 0.1386 - val_accuracy: 0.9526 - val_loss: 3.0127\n",
            "Epoch 124/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9770 - loss: 0.1473 - val_accuracy: 0.9356 - val_loss: 2.4556\n",
            "Epoch 125/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9731 - loss: 0.1003 - val_accuracy: 0.9594 - val_loss: 3.5263\n",
            "Epoch 126/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9837 - loss: 0.1251 - val_accuracy: 0.9614 - val_loss: 3.1247\n",
            "Epoch 127/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9858 - loss: 0.0567 - val_accuracy: 0.9602 - val_loss: 3.7653\n",
            "Epoch 128/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9825 - loss: 0.0633 - val_accuracy: 0.9452 - val_loss: 3.4260\n",
            "Epoch 129/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9752 - loss: 0.0964 - val_accuracy: 0.9506 - val_loss: 3.2321\n",
            "Epoch 130/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9680 - loss: 0.2424 - val_accuracy: 0.9354 - val_loss: 4.2815\n",
            "Epoch 131/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9708 - loss: 0.1626 - val_accuracy: 0.9476 - val_loss: 4.0921\n",
            "Epoch 132/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9716 - loss: 0.1181 - val_accuracy: 0.9520 - val_loss: 2.9938\n",
            "Epoch 133/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9761 - loss: 0.0920 - val_accuracy: 0.9542 - val_loss: 5.0256\n",
            "Epoch 134/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9747 - loss: 0.1184 - val_accuracy: 0.9398 - val_loss: 2.2068\n",
            "Epoch 135/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.9669 - loss: 0.1469 - val_accuracy: 0.9630 - val_loss: 6.9402\n",
            "Epoch 136/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9773 - loss: 0.1189 - val_accuracy: 0.9390 - val_loss: 3.7617\n",
            "Epoch 137/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9684 - loss: 0.1559 - val_accuracy: 0.9498 - val_loss: 3.1895\n",
            "Epoch 138/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9809 - loss: 0.0722 - val_accuracy: 0.9440 - val_loss: 4.3701\n",
            "Epoch 139/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9689 - loss: 0.1792 - val_accuracy: 0.9257 - val_loss: 3.7269\n",
            "Epoch 140/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9610 - loss: 0.1483 - val_accuracy: 0.9475 - val_loss: 3.9924\n",
            "Epoch 141/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.9785 - loss: 0.0753 - val_accuracy: 0.9602 - val_loss: 6.6040\n",
            "Epoch 142/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9806 - loss: 0.1111 - val_accuracy: 0.9505 - val_loss: 3.7418\n",
            "Epoch 143/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9823 - loss: 0.0994 - val_accuracy: 0.9535 - val_loss: 4.9357\n",
            "Epoch 144/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9711 - loss: 0.1750 - val_accuracy: 0.9479 - val_loss: 3.4763\n",
            "Epoch 145/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9744 - loss: 0.1217 - val_accuracy: 0.9339 - val_loss: 1.6539\n",
            "Epoch 146/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9735 - loss: 0.1119 - val_accuracy: 0.9487 - val_loss: 4.0479\n",
            "Epoch 147/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9759 - loss: 0.0857 - val_accuracy: 0.9575 - val_loss: 5.5662\n",
            "Epoch 148/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9686 - loss: 0.1391 - val_accuracy: 0.9348 - val_loss: 2.4153\n",
            "Epoch 149/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9677 - loss: 0.1010 - val_accuracy: 0.9449 - val_loss: 3.5717\n",
            "Epoch 150/150\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9662 - loss: 0.1320 - val_accuracy: 0.9340 - val_loss: 2.6528\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}